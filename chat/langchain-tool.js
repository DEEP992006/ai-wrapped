/**
 * ðŸ”§ LangChain Tool Example
 * Function calling using LangChain createAgent with Ollama
 */

import { configDotenv } from "dotenv";
import { createAgent, tool } from "langchain";
import { ChatOllama } from "@langchain/ollama";
import * as z from "zod";

// ðŸ” Load environment variables
configDotenv();

// ðŸ› ï¸ Define custom tool with schema
const getWeather = tool(
  (input) => `It's always sunny in ${input.city}!`,
  {
    name: "get_weather",
    description: "Get the weather for a given city",
    schema: z.object({
      city: z.string().describe("The city to get the weather for"),
    }),
  }
);
const getTemperatureTool = tool(
  async ({ city }) => {
    const temperatures = {
      'New York': '22Â°C',
      'London': '15Â°C',
      'Tokyo': '18Â°C',
      'Paris': '17Â°C',
      'Sydney': '25Â°C',
    };
    const temp = temperatures[city] ?? 'Unknown';
    return `The current temperature in ${city} is ${temp}`;
  },
  {
    name: "get_temperature",
    description: "Get the current temperature for a city",
    schema: z.object({
      city: z.string().describe("The name of the city"),
    }),
  }
);

// ðŸ¤– Initialize Ollama model
const model = new ChatOllama({
  model: "kimi-k2:1t",
  baseUrl: "https://ollama.com",
  headers: {
    Authorization: "Bearer " + process.env.OLLAMA_API_KEY,
  },
});

// ðŸ¤– Create agent with Ollama model and tools
const agent = createAgent({
  model: model,
  tools: [getWeather,getTemperatureTool],
});

// ðŸš€ Invoke agent and output result
const agentStream = await agent.stream(
  { messages: [{ role: "user", content: "what is the weather and temperature in New York" }] },
  { streamMode: "updates" }
)
for await (const chunk of agentStream) {
  if (chunk.model_request) {
  console.log("ðŸ“¤ Model Request:", chunk.model_request.messages[0].content);
  }
  if (chunk.tools) {
  console.log("ðŸ”§ Tool Response:", chunk.tools.messages[0].content);
  }
}
